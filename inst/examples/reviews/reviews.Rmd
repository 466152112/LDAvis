A topic model for movie reviews
========================================================

In this document, we fit an <a href='http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation' target='_blank'>LDA topic model</a> to the <a href='http://www.cs.cornell.edu/people/pabo/movie-review-data/' target='_blank'>Cornell Movie Review Data</a> introduced by Pang, Lee, and Vaidyanathan in their 2002 EMNLP paper, where we use 'polarity dataset version 2.0' (introduced in a subsequent Pang and Lee 2004 ACL paper). To fit the model, we used the R package <a href='http://cran.r-project.org/web/packages/lda/' target='_blank'>lda</a> and we visualize the output using <a href='https://github.com/cpsievert/LDAvis' target='_blank'>LDAvis</a>.

```{r setup, echo = FALSE, message = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE)
```

### The data
We created an R package, 'moviereviews', to provide the movie reviews data as an R object so that it's easy to load and start an analysis. The object `reviews` is a length-2000 list, where each list element is a character vector that contains a single movie review. The elements of a given character vector correspond to sentences in that particular review. Therefore, we can collapse these character vectors so that each review is contained in a single string. The new version of `reviews`, will be a length-2000 character vector, where each element contains a single movie review.

```{r read_new, eval = FALSE}
# moviereviews can be installed from GitHub via 'devtools::install_github("cpsievert/moviereviews")'
library(moviereviews)
data(reviews, package = "moviereviews")
str(reviews[1:2])
reviews <- sapply(reviews, function(x) paste(x, collapse = " "))
```

### Pre-processing
Before fitting a topic model, we need to tokenize the text. This dataset is already fairly clean, so we only remove punctuation and some common <a href='http://en.wikipedia.org/wiki/Stop_words' target='_blank'>stop words</a>. In particular, we use the english stop words from the <a href='http://en.wikipedia.org/wiki/SMART_Information_Retrieval_System' target= '_blank'>SMART information retrieval system</a>, available in the R package 'tm'.

```{r collect_stops, eval = FALSE}
# read in some stopwords:
library(tm)
stop <- stopwords("SMART")

# pre-processing:
reviews <- gsub("'", "", reviews)  # remove apostrophes
reviews <- gsub("[[:punct:]]", " ", reviews)  # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", " ", reviews)  # replace control characters with space
reviews <- tolower(reviews)  # force to lowercase

# tokenize on space and output as a list:
doc.list <- strsplit(reviews, "[[:space:]]+")

# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
```

### Using the R package 'lda' for model fitting
The object `documents` is a legnth-2000 list where each element represents one document, according to the specifications of the `lda` package. After creating this list, we compute a few statistics about the corpus:

```{r corpus, eval = FALSE}
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
```

Next, we set up a topic model with 20 topics, relatively diffuse priors for the topic-term distributions ($\eta$ = 0.02) and document-topic distributions ($\alpha$  = 0.02), and we set the collapsed Gibbs sampler to run for 5,000 iterations (slightly conservative to ensure convergence). A visual inspection of `fit$log.likelihood` shows that the MCMC algorithm has converged after 5,000 iterations. This block of code takes about 24 minutes to run on a laptop using a single core 1.7Ghz processor (and 8GB RAM).

```{r MCMC, eval = FALSE}
# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
```

### Visualizing the fitted model with LDAvis
To visualize the result using [LDAvis](https://github.com/cpsievert/LDAvis/), we'll need estimates of the document-topic distributions, which we denote by the $D \times K$ matrix $\theta$, and the set of topic-term distributions, which we denote by the $K \times W$ matrix $\phi$. We estimate the "smoothed" versions of these distributions ("smoothed" means that we've incorporated the effects of the priors into the estimates) by cross-tabulating the latent topic assignments from the last iteration of the collapsed Gibbs sampler with the documents and the terms, respectively, and then adding pseudocounts according to the priors. A better estimator might average over multiple iterations of the Gibbs sampler (after convergence, assuming that the MCMC is sampling within a local mode and there is no label switching occurring), but we won't worry about that for now.

```{r get_dists, eval = FALSE}
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
```

We've already computed the number of tokens per document and the frequency of the terms across the entire corpus. We save these, along with $\phi$, $\theta$, and `vocab`, in a list as the data object `MovieReviews`, which is included in the LDAvis package.

```{r save_list, eval = FALSE}
MovieReviews <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)
```

Now we're ready to call the `createJSON()` function in LDAvis. This function will return a character string representing a JSON object used to populate the visualization. The `createJSON()` function comptues topic frequencies, intertopic distances, and projects topics onto a two-dimensional plane to represent their similarity to each other. It also loops through a grid of values of a tuning parameter, $0 \leq \lambda \leq 1$, that controls how the terms are ranked for each topic, where terms are listed in decreasing of \emph{relevance}, where the relevance of term $w$ to topic $t$ is defined as $\lambda p(w \mid t) + (1 - \lambda) p(w \mid t)/p(w)$. Values of $\lambda$ near 1 give high relevance rankings to \emph{frequent} terms within a given topic, whereas values of $\lambda$ near zero give high relevance rankings to \emph{exclusive} terms within a topic. The set of all terms which are ranked among the top-`R` most relevant terms for each topic are pre-computed by the `createJSON()` function and sent to the browser to be interactively visualized using D3 as part of the JSON object.

```{r vis, results='hide'}
library(LDAvis)

# load the data here if the previous 3 blocks of code were not run to save time
data(MovieReviews)

# create the JSON object to feed the visualization:
json <- createJSON(phi = MovieReviews$phi, 
                   theta = MovieReviews$theta, 
                   doc.length = MovieReviews$doc.length, 
                   vocab = MovieReviews$vocab, 
                   term.frequency = MovieReviews$term.frequency)
```

Last, the `serVis()` function can take `json` and serve the result in a variety of ways. Here we'll write `json` to a file within the 'vis' directory (along with other HTML and JavaScript required to render the page). You can see the result [here](http://cpsievert.github.io/LDAvis/reviews/vis)

```{r serVis}
serVis(json, out.dir = 'vis', open.browser = FALSE)
```

