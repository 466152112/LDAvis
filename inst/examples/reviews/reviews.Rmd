Movie Review Data
========================================================

In this document, we fit an <a href='http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation' target='_blank'>LDA topic model</a> to the <a href='http://www.cs.cornell.edu/people/pabo/movie-review-data/' target='_blank'>movie review polarity dataset</a> shared by Pang, Lee, and Vaithyanathan (EMNLP, 2002, where we use 'polarity dataset version 2.0'). To fit the model, we used the R package <a href='http://cran.r-project.org/web/packages/mallet/' target='_blank'>mallet</a> and we visualize the output using <a href='https://github.com/cpsievert/LDAvis' target='_blank'>LDAvis</a>.

```{r setup, echo = FALSE, message = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE)
```

### Movie review data

We created an R package, <a href='https://github.com/cpsievert/moviereviews' target='_blank'>moviereviews</a>, to provide the movie reviews data as an R object so that it's easy to load and start an analysis. The object `reviews` is a list of character vectors where each character vector corresponds to a review. The elements of a given character vector corresponds to sentences in that particular review. Therefore, we can collapse these character vectors so that each review is contained in a single string.

```{r read}
# moviereviews can be installed from GitHub via `devtools::install_github("cpsievert/moviereviews")`
library(moviereviews)
data(reviews, package = "moviereviews")
str(reviews[1:2])
reviews <- sapply(reviews, function(x) paste(x, collapse = ""))
```

### Pre-processing

Anytime we fit a topic model, it's a good idea to do some pre-processing (cleaning of text). This dataset is already fairly clean, so we only remove punctuation and some common <a href='http://en.wikipedia.org/wiki/Stop_words' target='_blank'>stop words</a>. In particular, we use the english stop words from the <a href='http://en.wikipedia.org/wiki/SMART_Information_Retrieval_System' target= '_blank'>SMART information retrieval system</a>.

```{r collect_stops}
download.file("http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop", "stopwords.txt")
```

### Using mallet for model fitting

The R package mallet provides an interface to the java-based <a href='http://mallet.cs.umass.edu/' target='_blank'>MAchine Learning for LanguagE Toolkit</a>. To get started, we need to import our reviews along with our stop words and document names into mallet.

```{r import}
library(mallet)
instance <- mallet.import(names(reviews), reviews, "stopwords.txt")
```

Next, we initiate a topic model with 25 topics (a reasonable number of topics for a data set of this size, in our experience), load documents into that model instance amd extract the frequency of words that appears in the corpus.

```{r freqs}
model <- MalletLDA(num.topics = 25)
model$loadDocuments(instance)
freqs <- mallet.word.freqs(model)
```

It's generally a good idea to remove really infrequent words from the model's vocabulary before actually training the model. In this case, we add terms that appear less than 10 times to our list of stopwords and re-import the model instance.

```{r stops}
stopwords <- as.character(subset(freqs, term.freq <= 9)$words)
# 's' and 't' show up frequently and aren't very informative, so they are also included as stopwords
writeLines(c(readLines("stopwords.txt"), stopwords, "s", "t"),  "stopwords2.txt")
# Re-'initiate' topic model without the infrequent words
instance2 <- mallet.import(names(reviews), reviews, "stopwords2.txt")
model2 <- MalletLDA(num.topics = 25)
model2$loadDocuments(instance2)
```

Now we're in position to use mallet's train method to fit the topic model.

```{r fit}
# takes ~4.5 minutes on a macbook pro with 2GB RAM and a 2.26GHz processor 
model2$train(2000)
```

### Visualizing the model with LDAvis

To visualize the result using [LDAvis](https://github.com/cpsievert/LDAvis/), we'll need to extract a number of components from `model2`. Most important of all the "smoothed" estimates of the topic-term ($\phi$) and document-topic ($\theta$) distributions (smoothed means that we've incorporated the effect of the prior into the estimate). Also, note that the distributions should be normalized (since that is what LDAvis will expect).

```{r get_dists}
phi <- mallet.topic.words(model2, normalized = TRUE, smoothed = TRUE)
theta <- mallet.doc.topics(model2, normalized = TRUE, smoothed = TRUE)
```

There are a few more things we need from the training data. The `mallet.word.freqs` function will grab the vocabulary  (all the unique terms) and their observed frequency over the entire corpus.

```{r get_train}
freqs2 <- mallet.word.freqs(model2)
vocab <- freqs2$words
term.frequency <- freqs2$term.freq
```

We'll also need the number of tokens in each document:

```{r doc.length}
# TODO: is there a way to extract this info from the mallet instance?
tokens <- strsplit(reviews, "\\s")
stops <- readLines("stopwords2.txt")
tokens <- lapply(tokens, function(x) x[!x %in% stops])
tokens <- lapply(tokens, function(x) x[!grepl("[[:punct:]]", x)])
doc.length <- sapply(tokens, length)
```

At this point, we have all the inputs necessary to use `LDAvis::createJSON`. This function will return a character string representing a JSON object used to inform the visualization.

```{r vis, results='hide'}
library(LDAvis)
json <- createJSON(phi = phi, theta = theta, doc.length = doc.length, 
                   vocab = vocab, term.frequency = term.frequency)
```

Now, the `serVis` function can take `json` and serve the result in a variety of ways. Here I will write `json` to a file within the 'vis' directory (along with other HTML and JavaScript required to render the page). You can see the result [here](http://cpsievert.github.io/LDAvis/newsgroup/vis)

```{r serVis}
serVis(json, out.dir = 'vis', open.browser = FALSE)
```



