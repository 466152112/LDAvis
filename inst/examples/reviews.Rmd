Movie Review Data
========================================================

In this document, we fit an [LDA topic model](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) to [Cornell's movie review polarity dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/) using the R package [mallet](http://cran.r-project.org/web/packages/mallet/) and visualize the output using [LDAvis](https://github.com/cpsievert/LDAvis).

```{r setup, echo = FALSE, message = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE)
```

### Movie review data

The R package [moviereviews](https://github.com/cpsievert/moviereviews) provides the movie reviews data as an R object so that it's easy to load and start an analysis. The object `reviews` is a list of character vectors where each character vector corresponds to a review. The elements of a given character vector corresponds to sentences in that particular review. Therefore, we can collapse these character vectors so that each review is contained in a single string.

```{r read}
# moviereviews can be installed from GitHub via `devtools::install_github("cpsievert/moviereviews")`
library(moviereviews)
data(reviews, package = "moviereviews")
str(reviews[1:2])
reviews <- sapply(reviews, function(x) paste(x, collapse = ""))
```

### Pre-processing

Anytime we fit a topic model, it's a good idea to do some pre-processing (cleaning of text). This dataset is already fairly clean, so we only remove some common [stop words](http://en.wikipedia.org/wiki/Stop_words). In particular, we use the english stopwords from the [SMART information retrieval system](http://en.wikipedia.org/wiki/SMART_Information_Retrieval_System).

```{r collect_stops}
download.file("http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop", "stopwords.txt")
```

### Using mallet for model fitting

The R package mallet provides an interface to the java-based [MAchine Learning for LanguagE Toolkit](http://mallet.cs.umass.edu/). To get started, we need to import our reviews along with our stop words and document names into mallet.

```{r import}
library(mallet)
instance <- mallet.import(names(reviews), reviews, "stopwords.txt")
```

Next, we initiate a topic model with 25 topics, load documents into that model instance amd extract the frequency of words that appears in the corpus.

```{r freqs}
model <- MalletLDA(num.topics = 25)
model$loadDocuments(instance)
freqs <- mallet.word.freqs(model)
```

It's generally a good idea to remove really infrequent words from the model's vocabulary before actually training the model. In this case, we add terms that appear less than 10 times to our list of stopwords and re-import the model instance.

```{r stops}
stopwords <- as.character(subset(freqs, term.freq <= 9)$words)
# 's' and 't' show up frequently and aren't very informative, so they are also included as stopwords
writeLines(c(readLines("stopwords.txt"), stopwords, "s", "t"),  "stopwords2.txt")
# Re-'initiate' topic model without the infrequent words
instance2 <- mallet.import(names(reviews), reviews, "stopwords2.txt")
model2 <- MalletLDA(num.topics = 25)
model2$loadDocuments(instance2)
freqs2 <- mallet.word.freqs(model2)
```

### Model fitting/training

Now we're in position to use mallet's train method to fit the topic model. From the fitted model, we'll need a number of things to serve our visualization. Most important of all is the entire $\phi$ matrix where each column corresponds to a probability mass function over terms.

```{r fit}
model2$train(200) 
phi <- t(mallet.topic.words(model2, smoothed = TRUE, normalized = TRUE))
# You'll need normalized = FALSE to get the topic.proportions necessary for LDAvis
topic.words <- mallet.topic.words(model2, smoothed = TRUE, normalized = FALSE)
# 'count' of the number of tokens per topic
topic.counts <- rowSums(topic.words)
topic.proportions <- topic.counts/sum(topic.counts)
vocab <- model2$getVocabulary()
```

### LDAvis

LDAvis comes with a `check.inputs` function that ensures visualization inputs are valid. In addition, `check.inputs` also reorders the columns of the $\phi$ matrix according to the `topic.proportions`. Here we label these columns so that '1' stands for the topic that is responsible for the highest share of the corpus.

```{r vis, results='hide'}
# LDAvis can be installed from GitHub via `devtools::install_github("cpsievert/LDAvis")`
library(LDAvis)
out <- check.inputs(K = 25, W = length(vocab), phi = phi, 
                    term.frequency = freqs2$term.freq, 
                    vocab = vocab, topic.proportion = topic.proportions)
# Relabel topics so that topic "1" has highest topic proportion.
colnames(out$phi) <- seq_len(out$K)
```

Now that the "inputs" look OK, `createJSON` converts them into a JSON object that is used to drive the interactive visualization.

```{r createJSON, results='hide'}
json <- with(out, createJSON(K = 25, phi, term.frequency, 
                   vocab, topic.proportion))
```

Lastly, the `serVis` function will write a number of files including index.html which can be viewed in a web browser or embeded within other pages via an HTML `<iframe>`. See [here](http://www2.research.att.com/~kshirley/lda/index.html) for more info regarding reading the using the visualization. 

```{r serVis}
serVis(json, out.dir = 'vis', open.browser = FALSE)
```

<iframe src = "vis/index.html" width = "1200" height = "700"></iframe> 

